{
  "id": "a80a0830-b8c1-474a-8c03-d090da0b6ff2",
  "version": "draft",
  "meta": {
    "author": "Omnitool.ai team",
    "created": 1692906788314,
    "description": "No description.",
    "help": "",
    "name": "Chat with Article (Demo)",
    "pictureUrl": "omni.png",
    "tags": [],
    "updated": 1692908683731
  },
  "rete": {
    "id": "mercs@0.1.1",
    "nodes": {
      "539": {
        "id": 539,
        "data": {
          "author": "Omnitool.ai team",
          "button": "",
          "credits": "",
          "description": "No description.",
          "help": "",
          "title": "ChatPDF"
        },
        "inputs": {},
        "outputs": {},
        "position": [
          -1811.8831123535035,
          -752.0144247665279
        ],
        "name": "omnitool.recipe_metadata"
      },
      "543": {
        "id": 543,
        "data": {
          "audio": "",
          "documents": "",
          "images": "",
          "text": "Summarise the article into no more than 600 words in bullet points while retaining all key points. Give each bullet point a concise title or headline. Please use proper (and if necessary escaped) markdown formatting and ensure clarity and readability. Adding newline for each point. For example, \n---\n\n**Aim of the Paper**: \n- The paper aims to condense the fast-moving field of generative AI into easily digestible snapshots.  \n\n**Speed of Development**: \n- The speed of development in generative AI is exceptional and must be considered while reading.\n\n**Short-term Interactions Limitation**: \n- The value of short-term interactions with AI research is limited due to the overwhelming number of preprint papers released each week.\n.....\n---"
        },
        "inputs": {
          "text": {
            "connections": []
          },
          "images": {
            "connections": []
          },
          "audio": {
            "connections": []
          },
          "documents": {
            "connections": []
          }
        },
        "outputs": {
          "text": {
            "connections": [
              {
                "node": 546,
                "input": "prompt",
                "data": {}
              }
            ]
          },
          "images": {
            "connections": []
          },
          "audio": {
            "connections": []
          },
          "documents": {
            "connections": []
          }
        },
        "position": [
          -1208.2683212960733,
          -173.76254365219472
        ],
        "name": "omnitool.chat_input"
      },
      "545": {
        "id": 545,
        "data": {
          "text": "Generative AI: Background, Trends, Challenges and Opportunities,\nJuly 2, 2023\n \n About us\n● Georg Zoeller is co-founder of mercenaries.ai and a former Meta/Facebook Business Engineering Director for Financial Services, Commerce, Enterprise and Gaming.\n● Emmanuel Lusinchi is co-founder of mercenaries.ai and a former Meta/Facebook Partner Engineering Manager for VR .\nMercenaries.ai Pte Ltd. is a Singapore based technology startup providing expert consulting services in applied Artificial Intelligence. Our team consists of former Big Tech engineers, solutions architects and partnerships/policy experts.\nWe also develop “omnitool”, an “AI Lab in a Box” currently in technology preview helping customers to rapidly explore and prototype with a broad variety of AI tools both cloud hosted and on the edge. Interested parties can reach out to sales@mercenaries.ai for more information.\nPart 1 - Background, Current Trends 3\nGenerative AI - A large technological leap 3 Steady, accelerating progress 4 From Research Lab ML to Applied AI 5 Trend: Open Source vs Closed Source 6 Trend: Edge vs Cloud 9 Trend: US vs The World Bifurcation 11 Talent Market Segmentation 13\nSegment 1: Deep Generative AI Research Talent 14\nSegment 2: Applied Generative AI Talent (“AI Engineers”) 15 Compute availability: A near term concern 16 Hardware and Supply Chain Overview. 16 The Importance of Software Stack 17 Power / Electricity Availability and Cost 18 Growth Hacking like it’s the 2010s 20 Compute related safety Issues 21 Generative AI favours incumbents 22 The Changing Value of Data 23 Proven Value 25 Not all data is valuable 25 Changing posture towards data 26 “There is no (long term, technical) moat in AI” 27 Capitalising on Efficiency Gain 29 Public Relations Challenges 30 The tallest challenge: Staying Current. 32 Summary Part 1 and Closing Thoughts 35 Key Acronyms / Terms 36\n   \n Part 1 - Background, Current Trends\nGenerative AI - A large technological leap\nThe speed at which generative AI has developed over the last two years has been described as breathtaking and exponential and it is worthwhile to unpack what drives the acceleration:\nGenerative AI represents the first large technological leap in the history of mankind that is almost entirely unbounded by logistical constraints: Where it took previous leap technologies like electricity, telephone, cell phones, internet or mobile internet decades of investment into physical (i.e. laying cables, building repeating towers) and virtual infrastructure (i.e. App Stores, cloud foundation and networking) to reach broad market adoption, few such constraints exist for generative AI.\nIndeed, Generative AI takes full advantage of existing fibre networks, global data centres, existing app stores, the global web and well established standardised computer-to-computer interfaces such as the REST API. Due to this, the cost and effort required to integrate Generative AI into existing products is minimal, and Generative AI capabilities are rolling out faster than any preceding technology.\nAs a result, time to market is compressed, putting pressure on regulators and market participants to grapple with the effects of the technology.\nThe following capabilities exhibited by generative AI models are driving this change:\n- Ability to handle unstructured, imperfect human language input and engage in unstructured dialog (“prompting”).\n- Ability to create / perform creative tasks such as writing, image synthesis, music composition and writing code approaching and sometimes exceeding human performance.\n- Ability to summarise, synthesise, contextualise and compress knowledge.\n- Ability to abstract and transfer between formats and contexts such as translation between languages, programming languages, writing styles, perform perspective shifts and\ntranspose data formats\n- Indistinguishable from human audio synthesis, faster than real time transcription and\ntranslation.\nUnlike\ngeneralised and often performs well even on tasks it was not specifically trained or optimised for. The general mechanisms by which this happens, or even how to measure this process (dubbed “emergence”) are unclear and contentious, the real world observable capabilities however are not.\nprevious generations of AI models, this new breed of system is able to operate more\n\n Steady, accelerating progress\nIt is important to note that research in Generative AI has been advancing and producing breakthroughs for years, but required an ‘iPhone Moment’ in the form of Stable Diffusion and ChatGPT’ to enter public and investor consciousness as a commercial opportunity:\nPrior to 2022, Generative AI had already been making steady progress for several years. StyleGan, capable of creating human faces was released in 2018, GPT-2 in 2019 and the advent of transformer technology (Data is all you need, 2020) indicated significant breakthroughs but were masked by pandemic distractions and market distortion.\nhttps: //towardsdatascience.com/ten-years-of-ai-in-review-85decdb2a540\nWhile the difference in dataset and core between GPT 3 (released June 2020) and GPT 3.5 aka ChatGPT (released November 2022) objectively was rather minor (approximately 150.000 sentences of RLH training), it resulted in vastly better accessibility of the product. Combined with Microsoft’s aggressive, Go-To-Market strategy via Bing and positioning ChatGPT as a consumer product, OpenAI managed to capture significant market share, investor imagination and propelled their brand and product into the centre of attention.\n     \n From Research Lab ML to Applied AI\nSince 2013, the majority of AI research and development primarily happened in Big Tech labs like\nFacebook (FAIR), Google Deepmind and Google Brain, concentrating the world’s top talent.\nMost importantly though, much of the research and breakthroughs made in these labs were not commercialised as neither company could reason out the incremental opportunity over their existing, (pandemic booming) ads business models. While scientists were allowed to publish their research, internal incentives at both companies strongly aligned with driving key business metrics. Organisational design (Labs as Ivory tower, separate organisation) prevented broader diffusion of the technology into the general engineering population.\nAs a result, breakthrough technologies rarely found product managers willing to risk sponsorship and resources on bringing these technologies to market unless they specifically drove key business metrics and were incremental and aligned to key business priorities (ads).\nFacebook/Meta specifically had another problem: Despite commanding the world’s largest number of A100 GPUs in 2020, they faced serious compute capacity crunch from their efforts to undo the damage inflicted by Apple’s Signal blackout with IOS 14.5, the solution to which was massive investments in AI capacity, leaving little capacity for compute intensive plays like generative AI.\nBy 2019, the companies defining much of the technological innovation of the 2010s had become gatekeepers of technology rather than innovators, unable to escape the gravity of their existing, very profitable business model, at least while it was growing at pandemic inflated rates. When attempts at capturing or manufacturing ‘additive’ technological growth narrative (crypto/web3/metaverse) failed and the ads model started to run into its post-pandemic reckoning, the hold on the talent in the space started to break as well.\nUltimately, OpenAI benefited tremendously from the brain drain and acquired from these labs talent and several of their ignored technological breakthroughs. It enabled them to drive their products to commercial viability and cement their current leadership position, but also diffused talent and know-how across many of the other AI startups such as Anthropic. (See also: Talent Availability”)\nAnd while Facebook and Google have since pivoted hard to ensure investors understand AI as their future growth opportunity, much of their once proprietary knowhow has found its way to competitors and open source by now. (See Part 2 - Meta, Google)\n    \n Trend: Open Source vs Closed Source\nThe emergence of Open Source in the Generative AI scene has been another, significant accelerant of progress. Up to then, Talent, high inference cost and hardware requirements for training and the lack of available weights prevented outside parties from engaging with the technology.\nStability AI’s (also see Part 2 - Stability AI) disruptive move to open source Stable Diffusion last August re-shuffled the rules of the ecosystem and can be considered the iPhone Moment of Open Source AI.\nBefore the release of Stable Diffusion, contributors to the field of generative AI had been confined to the labs of Big Techs and a few well funded companies like OpenAI. Stable Diffusion’s release to a ready and waiting audience instantly added a truly massive number of capable technical contributors to the field and triggered an exponential growth in technical breakthroughs.\nThis can be visualised by the near vertical rise of the Stable Diffusion github repository (see pale blue line in the chart below) in contrast to very popular repos such as Bitcoin (gold) or Ethereum (purple)\nTechnical contributor interest (Github Stars) of Stable Diffusion vs other technologies over time\n  \n The addition of a vast number of contributors to Stable Diffusion had immediate effects. Following the code release on Github:\n● Within weeks of release, Stable diffusion was able to run on low end consumer graphics cards and Apple hardware.\n● Within weeks, inference speed went from 1 image in 6 seconds to sub second generation speed on higher end systems.\n● Within months, it was ported to mobile devices.\n● The emergence of Dreambooth and LoRA finetuning moved the ability to fine tune custom\nmodels to consumer machines and cut time for fine tuning by orders of magnitude.\n● 6 months after release, the release of ControlNet allowed incredibly high controllability of generations, making the technology viable for many commercial use cases.\n● 9 months after release, few output fidelity challenges remain, human hands and faces can be produced perfectly and rivalling the top end proprietary model, Midjourney.\nThe combination of Big Tech training capabilities and Open Source has been particularly potent in overcoming incumbent moats: When Meta ‘released’ the weights to their LLaMA LLM in February, the same dynamic observed with the release of Stable Diffusion repeated:\n- Within weeks of release, the model, previously running on top end industrial GPUs had received efficiency improvements enabling it to to run an a wide variety of consumer hardware, including Consumer GPUs, Apple Hardware, iPhones and even Raspberry Pi\n- New methods of Training and Fine tuning, such as Low Rank Adaptation were implemented, allowing for effective training in minutes rather than hours and days.\n- Within months of release, Open Source models derived from the original LLaMa weights such as Vicuna, Falcon, by available benchmarks, may have already overtaken Google Bard and are closing in on the #2 best performing model in the market, GPT 3.5 (ChatGPT) across many dimensions - while requiring vastly lower hardware resources for inference.\n     It is important to note that Open Source is not limited to volunteer, spare time contributors,\n adding value but can be seen as a permanent ecosystem bifurcation, splitting the generative field\n into a proprietary focused side (OpenAI, Google, Anthropic, Midjourney, etc) and Open Source\n (Meta, Apple, Stability, EleutherAI) with some companies like AWS and Microsoft hedging their\n bets on both sides.\n       \n  LLM Benchmarking from LMSys showing proprietary models still leading on most topics,\nIn fact, almost every popular proprietary, generative AI offering is now seeing open-source competitors emerging, which, in some cases (Dall-e) are, at least temporarily, eclipsing the capabilities of these models.\n○ Midjourney, Dall-e (proprietary): Stable Diffusion (open source)\n○ Github Co-Pilot (proprietary): Star Coder, P1 (open source)\n○ ChatGPT (proprietary): Stable Vicuna, WizardLM, Falcon (open source)\nWhile the leaked Google internal memo (“We have no moat, and neither does OpenAI”) should not be seen as indicative regarding the position of the company, we believe that it mostly captures the significant challenge Open Source poses to the proprietary approach both Google and OpenAI have been taking.\nWe do not however support the conclusion Open Source will be able to overtake commercial top end foundational models trained by large tech companies or governments, which will likely require increasingly massive supercomputer scale hardware infrastructure to train and operate. It is however likely that Open Source models will come to dominate the field of lower end inference\nneeds, specialised use cases (due to powerful fine-tuning capabilities) and on-device / on-edge inference - which is another emerging field (see below)\n       Ultimately, Open Source presents a new market force constraining the ability for proprietary\n players to build technological moats and to dictate pricing by offering alternatives, especially\n for use cases requiring less than top-end performance.\n \n Trend: Edge vs Cloud\nPowerful AI, for the last few years, has been a datacenter and proprietary models play. Even a year ago, the ability to run local generative AI capabilities for all but the most well provisioned companies was severely constrained by access to Talent, Knowledge, Hardware and Model Weights.\nThe most effective (and often only) way therefore to commercially exploit powerful, expensive generative AI models (especially given compute pricing - see Part 2 - NVidia) has been providing cloud based inference endpoints for customers. This model also offers the added benefit of being the only way of keeping ‘a secret sauce’ in AI (albeit far from perfect) As seen with the LLaMa weights “leaking” (or Novel AI’s model leak in 2022) - once model weights are out in the wild, they cannot be taken back (like any other digital file).\nAs mentioned earlier, this low friction nature of API-As-A-Service Generative AI is fueling much of the current wave of AI products and tools and is allowing even companies with moderate resources to quickly and effectively overcome adoption challenges. It appeared, for a while, as if AI would be a primarily cloud based opportunity.\nThe last few months however have given a number of indications that forecasting such cloud supremacy in AI may have been premature and that on-device/on-the-edge AI will play an increasing role in the overall ecosystem.\n● The rise of Open Source AI has significantly shifted the hardware requirements for running powerful generative AI applications, in some cases as ridiculously as running a large LLM like LLaMa on a 30$ Raspberry Pi (albeit slowly). It is now viable for even small companies to run mid-sized LLMs and top-end diffusion models on premise using consumer hardware.\n● Apple is a company with much to lose in hardware investments and App Store payments should AI move economic value creation to cloud endpoints. It is not surprising therefore to see that Apple not only ported Stable Diffusion to CoreML stack, but also squeezed the model onto the iPhone, hinting at future on-device capabilities.\n● Data protection, privacy considerations, fear of data being misused for training continue to rank high as core constraints on AI adoption by corporate decision makers. Uncertainty about cross-jurisdictional data transfer adds to the hesitation. Local inference and training capabilities are the most effective tools to mitigating these concerns. Especially given Apple’s strong focus on privacy messaging and the EU’s “Privacy regulation as a (profitable) market protection measure” approach already seen with GDPR, we can expect this angle to be of increasing importance.\n       \n ● Progress on fine-tuning has been significant, enabling even smaller models to reach and sometimes eclipse the performance of foundational, generalised models for specific, tightly constrained use cases.\nAdditionally, we can expect, once generative technology has stabilised onto common architecture, that hardware improvement with specialised AI chips will radically improve local inference capabilities in a similar way as AV encoding/decoding and photography specific hardware has on current devices.\nIn the end, we predict that both modes of AI delivery, on-device (or ‘edge’) and cloud-based, will become ubiquitous and many systems will converge on hybrid operation:\n● Edge AI for low cost, time sensitive, privacy relevant (such as PII redaction) or purpose trained specialist tasks\n● Cloud-based AI for complex or state-of the art use-cases - at premium pricing.\nOne (of several) possible split to consider in this context is per-frame applications versus event\ndriven applications:\n● Per frame applications are usually found in entertainment verticals such as gaming and video. It is likely that this segment will be dominated by on-the-edge / on-device compute for reasons of sensitivity to latency and scalability. Per frame capable visual diffusion capabilities have significant potential to remake much of the entertainment industry in the coming years.\n● Event driven applications, such as apps solving specific problems are better suited for cloud endpoints due to higher demand on accuracy. While incumbents in apps and productivity software are unlikely to give up their field willingly, we see significant longer term disruption risk from chat-based, multi modal interfaces.\nOne challenge with forecasting the impact of edge and hybrid AI on the overall trajectory of generative AI is that regulatory conversations around the world appear to be primarily happening under the impression of large models like ChatGPT and have not caught up yet with these developments.\nDreams of regulating, controlling and de-risking generative AI ‘The China Way’ by allowing concentration on few, large manageable companies through natural market forces (tolerated, encouraged monopolies) have always been unlikely, but appear entirely unrealistic without massive collateral damage given the last four months of development (which does, of course, not imply it won’t be attempted anyway)\n\n Trend: US vs The World Bifurcation\nLess than a trend and more of a continuation of previous two decades, the Silicon Valley based US technology industry continues to dominate technology, including AI:\nAll of the top AI companies with the exception of Stability (London, UK) are US based:\n● OpenAI (Silicon Valley)\n● Anthropic (Silicon Valley)\n● Google (Silicon Valley)\n● Meta (Silicon Valley)\n● AWS (Silicon Valley\n● Apple (Silicon Valley)\nAs are the hardware makers\n● NVIDIA (Silicon Valley)\n● AMD (Silicon Valley)\n● Google TSU chips (Silicon Valley)\n● Apple M2 chips (Silicon Valley)\nWhile China is another leading source of Artificial Intelligence research in the world, the recently enacted US Chip ban has at least temporarily limited the country's ability to compete in the global market - a situation unlikely to improve anytime soon, if anything further export restriction and economic sanctions in this space can be expected.\nSpeaking to experienced individuals in tech company leadership, VC circles and university research in Silicon Valley, the general sentiment is that the US has, leveraging their overwhelming incumbent bonus (see below) from existing technology, decisively “won” in AI and will likely lock in much of the economic gains expected from AI in the coming decade.\nOn the face, this does not seem like an unreasonable assessment at all, especially given historic events and when observing the ruthlessly efficient GTM strategy and lobbying power projected by OpenAI (see Open AI - Part 2).\n\n We do see indication that things won’t be as clean of a rout as Silicon Valley imagines, for several reasons:\n● Historically low trust in the benefits of globalisation and experiences over the pandemic leading to increasing movements toward decoupling, de-risking and globalisation and demonstrated willingness to put the thumb on the regulatory scale (see below).\n● Significantly increased experience by policy makers based on a decade of attempts at reigning in Silicon Valley based companies (Social Media, Uber) through regulation, including far reaching, confrontational privacy in Europe (GDPR), Data localisation and competition law in India.\n● Significant concerns about biases, cultural representation and values. It appears likely that LLMs will be seen as the next battleground for identity and politics and as such inviting regulatory intervention and attempts at value shaping. The pressure of these conversations will only increase towards the next US election cycle, especially once Elon Musk enters the conversation with his current efforts building an ‘uncensored’ LLM.\n● Trends towards Open Source and Edge AI challenging the notion that a few, US company controlled data-centres will be able to gate access to most AI capabilities.\n● Serious national sovereignty concerns posed by AI (Also see our LLM Paper) - especially in light of historic US attempts at export controlling technology and recent trade power projection against China (5G / Chip war) and Russia (SWIFT). Few countries have a significant appetite for effectively handing out to US based companies a kill switch to a significant portion of their knowledge-based economy.\n● Disillusion and discontent with Big Tech following a decade of increasingly challenging social issues and recent aggressive value extraction moves in the market.\nWe are observing increasing interest from national governments at addressing US supremacy in this space, including\n● Development of nation foundational model efforts\n(We continue to believe that while engaging in large scale foundational training is better left for the future,capability building should be pursued in the present. Whether these models can succeed without some regulatory coercion however is another matter altogether)\n● Attempts to address talent shortfall and attract investments, such as Canada’s recent law changes to specifically target laid off H1B workers in the US.\n    \n Mistral.ai’s feat of raising a 113M investment on little more than a team should be seen under this light. While the narrative of exceptional talent (“only a few hundred people in the world who have the knowledge to build foundational models”) is compelling and not entirely removed from the truth, it is also plausible that private investors, including notable figures like Google's former CEO Eric Schmidt, were also motivated by other factors.\nThe potential for increasing national favouritism or protectionism, and the threat of stringent EU regulatory measures that could effectively divide the AI landscape and limit US entities’ access to Europe could very well also have played a role in enticing their investments as a hedge.\nTalent Market Segmentation\nWe observe that many decision makers struggle with understanding the talent situation in Generative AI, confused by the sudden transition from Machine Learning narratives to Applied Generative AI.\nWe suggest segmenting the field into two distinct talent markets of relevance, and recommend the following attempt at describing the emergent talent profile in this space (source).\nhttps: //www.latent.space/p/ai-engineer\nNew job profiles do periodically emerge in tech, driving change and, often career disruption (e.g. On Premise IT Administration to Cloud Operations) and it takes time for profiles to settle competing narratives and power struggles within organisations and research that accompany them\nWe are supportive of the attempted definition of “AI Engineer” made in the above linked article as it, so far, matches the observed reality (and talent) in the field , while other, competing attempts, such as “Prompt Engineer” have failed to develop traction at this point.\n   \n Segment 1: Deep Generative AI Research Talent\nThe ‘FAIR/ Google Brain/ Deepmind/Etc Research Leaders” people, long term AI research scientist with a deep background in Machine Learning and, as of late, foundational model development.\nRequired for:\n● Building competitive top end foundational models from scratch\n● Chance of significant scientific breakthroughs to retain competitiveness on the top end\nKey Skills:\n● Building / Running Large ML Research and Production Operations\n● Foundation Model Scale Data Ops\n● Developing iterative improvements, breakthroughs (e.g. better fine tuning methods)\n● Familiarity with research and production operation\n● Operating top end models\n● Training augmented models\nThe ‘common knowledge’ in the field is that there are only about 200 individuals with the deep knowledge of generative AI to build foundation models from scratch and they all live in Silicon Valley or would move there” is likely off by a few hundred, but generally in the right ballpark.\nStanford famously brags about a 0% rejection rate for top research talent, adding to the myth here. The yearly total compensation (TC) range for these Senior Researchers in this field is easily in the multi-million USD, with strong retention incentives.\nIn our estimation, dislodging these people is incredibly hard and, without the ability to also poach their immediate team and resources (data!) as well, not likely to result in any meaningful competitive success in the short term.\nA second tier in this area is more achievable: Data Scientists and ML engineering leaders with several years of experience operating in these labs. An order of magnitude larger cohort and more accessible but still priced in the 1-3M USD TC range and at least some of them are on the market, however briefly.\nThis cohort could be achievable for well provisioned companies, research organisations and governments and enable bootstrapping of specialist teams and, leveraging Open Source progress and resources available through expert outfits like Stability, MosaicML, TinyGrad, may be able to mount competitive mid-to-top tier efforts at augmented specialist models in the coming years.\n\n Segment 2: Applied Generative AI Talent (“AI Engineers”)\nAs discussed before, much of the work in the AI field before 2020 was in research (and much into technologies effectively. obsoleted by generative AI). Combined with the rapid development in the last year, the inevitable “5 years of applied generative AI experience” job postings appearing now are missing the mark.\nIndeed, much of the new “Applied Generative AI” talent pool is self-taught and motivated, has significant experience with the surrounding technologies and entered the field only in the last 6 to 18 months - often after a layoff from the Big Tech companies and is in strong demand globally.\nKey Skills:\n- Ability to stay on top of a rapidly evolving field\n- Hands on experience Evaluating, Optimising and Deploying models\n- Hands on experience training smaller models, LoRas, RLHF\n- Hands on experience utilising the emergent, rapidly changing tooling in the field.\n- Hands on experience Understanding and ability to use state of the art fine-tuning\n- Applying generative AI to existing businesses and business models.\n- Some hands on experience to last-generation ML (ranking, gradient descent, GANs, etc)\nSelf-taught and Hands-on being the key distinguishing factors as, at this point in time, there are simply no institutions teaching Applied AI (bootstrapping of which should be a key priority for governments at this point).\nWhile this is potentially a tough sell in ‘education achievement’ driven locations like many Asian countries, the good news is that Applied Generative AI talent is imminently trainable / reskillable, because most of the relevant technology is less than a year old and leveraging it does not require deep scientific background or even traditional machine learning background.\nBusinesses need to take advantage of the recent and on-going progresses in AI to stay globally competitive. For these businesses, acquiring talent from Group 2 for the purpose of knowledge transfer is a viable strategy , as is identifying key candidates for reskilling. We suggest companies retain external expert talent to develop definitions and talent acquisition strategy targeting this cohort.\nFor the same reason, broad Applied AI Education itself will become increasingly important over the coming months and years as the only short term means of extending the advantages of applied AI to most businesses.\n  \n Compute availability: A near term concern\nHardware and Supply Chain Overview.\nWhile we noted that Generative AI adoption is mostly unconstrained of logistical factors, it is likely that rapid adoption of the technology will be tempered by hardware and compute availability:\n● At present, specialised compute GPUs from NVIDIA represent the vast majority of training (the only other viable training hardware at present is Google’s TPU) and much of inference hardware in use in the world and performance jump between generations of these devices often exceed an order of magnitude. It is not incorrect to say that NVIDIA currently is the pricemaker of the entire industry. (More details on this topic can be found in our summary of Nvidia in Part 2)\n● Nvidia itself is completely dependent on TSMC upstream, which shows the relative fragility of the supply chain. Apart from geopolitical risks of black swan events in the South China sea, other factors such as shutdowns of shipping lanes, malware attacks, or natural disasters could cause severe, temporary market challenges, especially for generative AI companies reliant on hardware..\n● Nvidia being a US headquartered company at this pivotal nexus of technology, also creates potential risks for economic sanctions or export regulation, at least for top end cards. While we don’t think calls to slow down or regulate AI by means of GPU supply chain controls have much of a chance of becoming reality, it is important to remember that the US has a long history of leveraging technology export restrictions when it serves government ambitions, from the 1990s crypto export limitations to more recent limitations on chip and mobile technology. The current crop of restrictions towards China is likely to grow as well.\n● In a blog post, since removed at the request of OpenAI, CEO Sam Altman was quoted saying that the company’s ability to expand features such as access to higher end models was already severely constrained by their ability to grow compute capacity.\nWe predict that in the short to mid term, GPU hardware availability will likely become a key constraint on corporate and government AI foundational training ambitions and may end up driving up AI inference costs as well. The market appears to understand this risk: Ever since Stability AI kicked off the habit of announcing Nvidia GPU cluster size along investment rounds to signal ability to execute on their ambitions, we can observe an accelerating trend of rapidly increasing AI investments accompanied by hardware announcements ( the recent 1.3B round by\n      \n Inflection included a 22.000 H100 GPU cluster announcement - for reference, a single H100 is priced at ~USD $30.000-$40.000).\nCompanies unable to secure sufficient compute capacity, especially in the training / foundational model space, may find themselves unable to capitalise on market opportunities.\nOn the inference side the situation is not as one-sided as on foundational training, with more options (such as Qualcomm chips) existing to provide services - but it seems likely that the market will run into at least temporary compute crunch there as well as much of the competition in the space is currently fought using a “acquire users through free compute” playbook (see: Market Dynamics: Growth like it’s 2010s)\nThe Importance of Software Stack\nChallenging NVIDIA’s dominance in this field is a tall order as their decade long investments into AI give them a commanding lead and the deep deep integration into the software stacks powering the field (CUDA) provides a formidable moat. That said:\n● With increasing costs and demand, supply limitations and NVIDIA increasingly entering their own downstream market through products and direct investments (see Part2), incentives for competitors to develop alternatives increase. Many in the industry keenly remember the last round of GPU shortages driven by NVIDIA’s sudden shift to serve the crypto market and have a vested interest in avoiding dependency and building resilient supply. While little can be done to address these challenges in the short term, we do expect escalating investments in alternatives to Nvidia, especially on the training hardware side.\n● Apple has invested significantly into their CoreML framework leveraging the M1/2 architecture’s powerful unified memory design to enable machine learning inference load and embracing Open Source has made a measurable impact even in a short amount of time. Key open source contributors like Georgi Gerganov along with tens of thousands of others have, in the last few months, made Apple Silicon a viable platform for AI development and local, on-the-edge inference.\n● AMD, Nvidia’s most competitor on the GPU market has announced ambitious plans for AI Accelerator hardware and Meta, who successfully re-engineered the datacenter hardware market once before with their open source designs, has already announced their first custom AI training accelerator chip. But while some companies such as MosaicML have made significant progress this month in deploying AMD hardware in AI, software stack support is significantly lagging behind and there is little indication of open source engagement\n     \n We note another, powerful effect of open source here: The software stack in AI is almost completely dominated by Open Source products. Pytorch (maintained by Facebook/Meta) is Open Source and aims to support any available hardware, but relies on contributors to do so. Popularity of Apple hardware among software engineers ensured almost instant support in the framework on M1/M2 architecture. Pytorch famously outcompeted Tensorflow in large parts due to Google’s decision to focus on supporting their own TPU hardware preferentially.\nOpen source contributors, numbering in the tens of thousands of contributors, also, within weeks of release, created first party ports of key generative technologies like whisper and llama, with very limited support from Apple. AMD on the other hand, lacking an equally engaged audience and suffering from reputation challenges among developers, currently has to build software stack support for their hardware with vastly fewer resources, mostly in form of paid employees.\nPower / Electricity Availability and Cost\nGPU Compute is, at least compared to other forms of computation, electricity intensive. Data centre companies have likely forecast some level of increased GPU demand into their plans, but still have likely been caught flat footed by the sudden increase in top compute demand.\nIn some regions and locations, rapid expansion of compute capabilities will face logistical challenges in procuring the necessary power supply guarantees, creating localised growth limitations.\nIt is important to note that there are no official numbers on the inference cost of OpenAI’s top end models (which are the most used models in the world), but they likely in the ballpark of USD 700k-1M a day at this point, the majority of which would be attributable to GPU Cost with Power cost coming second.\nCountries, like Singapore, who may have recently slowed down their compute and data centre investments under the impression of rising power prices and the realisation that they provide few job opportunities for the space and energy they consume, will likely have to reevaluate their options as the ability to supply businesses with the necessary compute capacity will increasingly become a national sovereignty concern and competitive advantage. The question of space consumption by data centres vs. jobs created may have to be revisited with a different perspective too.\n    \n In the long run, GPU compute will likely emerge as a critical resource required for the economies of developed nations to compete and maintain national sovereignty. Parallels with the rise of electricity suggest that, at some point in the future and given broad penetration of AI based technologies into the consumer market, compute capacity may rise to the level of public utility status.\nIn this context, the economic opportunity in efficiency and climate friendly technologies should not be underestimated. Even if one doesn’t subscribe to the notion that AI adoption should be seen in Darwinian terms, continuously increasing adoption may be unavoidable,\nIn any case, mounting pressure to address power supply resilience, ecological sustainability and climate/carbon concerns created by rapidly increasing AI compute is guaranteed. Technologies like Optical Transformers are worth watching in this context.\n       As a side note, it seems likely that these dynamics will spell the ultimate death knell for any form of computationally intensive crypto systems or ambitions due to pure opportunity costs.\nNVIDIA, who were more than happy to sell specialised mining GPUS to the point of cannibalising their own games business in the last decade, have already made clear that AI is more promising in their eyes.\n  .\n\n Growth Hacking like it’s the 2010s\nLooking at the AI SaaS market specifically, most players, including leaders like OpenAI, currently follow the tried Silicon Valley playbook of paid customer acquisition - the main currency being ‘free AI compute credits’, for several reasons:\n● GPU hardware being the main expense, the best way to amortise the significant up-front investment these cards represent is to continuously saturate them with compute load, 24/7, which requires significant user-numbers and logistics to overcome peak usage challenges.\n● Existing Users representing the closest thing to ‘a moat’ in the market (see below)\n● Investors continue to put a significant premium on user growth as a success metric.\n● And, in the case of OpenAI’s ChatGPT, a proxy battle between Microsoft (Bing) and Google (Search) where success is measured in users.\nA hardware supply crunch sending prices upwards would have serious implications in this context, especially for less established players and we question whether the 2010’s tech growth playbook of ruinous user acquisition battles still makes sense in this context (see also Part 2 - Stability AI’s motivation for going Open Source).\nMuch has been said about ChatGPT’s ability to rapidly grow to 100M users, (we want to stress that these numbers have been reported by external companies and OpenAI itself has not confirmed or qualified these numbers), but when each account comes preloaded with USD 10$ of compute credits - enough for 100s of useful chatGPT question - it’s also not very surprising to see significant account creation activity.\nMost other AI services currently follow the same ‘move fast and grab users at any cost’ strategy. With rapidly developing technology providing little moat to businesses, user growth numbers are still seen as a key success metric by many investors and everyone, from successful chatbot aggregators like Poe.com to Stability.AI’s offerings and the strategy certainly has worked for more companies than just ChatGPT:\n● Jasper AI (AI writing) is reporting $75m ARR after 2 years and offered a generous free trial before.\n● Midjourney (AI images) is estimated to have reached ~$80m ARR and bootstrapped with generous allowances and initially free tier availability.\n● GitHub’s Copilot is estimated at 50-80m ARR via their 10$/mo subscription after initially free trial period\n  \n Compute related safety Issues\nWhen a company is giving away free money, especially convertible into an universally useful product like ChatGPT API calls, they naturally attract significant interest from bad actors. Given OpenAI’s substandard integrity protections for the first months of ChatGPT availability, the almost unrestrained ability to create sockpuppet accounts for future use in scams and nefarious use was likely leveraged at significant scale. ChatGPT, after all, is incredibly useful for tasks like creating fake reviews, spam, phishing and social media influence campaigns.\nIn this context, we also note that by June, OpenAI was considered one of the prime destinations for converting stolen credit card credentials (into AI compute). While never officially confirmed, (but experienced first hands and validated as an at scale problem by us through conversations with insiders) local banks like DBS went as far as cancelling many standing subscriptions and preventing new sign-ups to ChatGPT due to the extent of the problem. We note (from first hand experience working many years in the field) that credit card and sign up fraud is a well understood problem for Big Tech and that the kind of issues experienced by OpenAI are more likely due to intentional de-prioritisation of safety measures in favour of growth than evolving adversaries.\nIt appears, conveniently demonstrated by these fraud challenges, several rounds of data leaks, and the disastrous (from a security perspective) launch of ChatGPT plugins, as if ‘move fast and break things’ continues to be alive and well in Silicon Valley’s current AI race and that safety is not a particularly concern.\nWhile calls for AI safety are echoed from many companies in the field, we believe that these are primarily driven by (anti)competitive and regulatory shaping motivations rather than actual concern and that action, or rather inaction on the existing, understood safety issues indicate that much of the last decades playbook of heavily focusing on growth numbers to dazzle investors continues to be in effect.\nThe safety debt incurred during this race to attract and lock in users is likely to come due in the future. With AI compute in increasing demand and essentially equivalent to cash, training weights representing much of the value of most companies and training data in increasing demand, it is hard not to predict a significant wave of serious data breaches, computer theft and even failure of entire companies due to loss of their proprietary training weights.\nFrom an investment perspective, Cyber-security vendors, Security aligned Infrastructure companies like Cloudflare and new players specialised in Data Loss Prevention for AI training artefacts or personal data protection compliance are relatively safe bets when it comes to growth, especially taking into account the exponentially increasing amounts of data required to fuel AI ambitions.\n(See also our paper Generative AI - Safety Issues, Red-Herrings and Regulatory developments)\n    \n Generative AI favours incumbents\nWhile incumbents in many industries tend to be slower moving than up-start competitors, the specific situation around Generative AI - low time cost of initial entry via APIs, high cost of talent and hardware needed to develop foundational models currently favours incumbents over challengers.\nA company with a large, established user base and distributed product moving with sufficient expediency to adapt AI capabilities into their products has little to fear in terms of disruption at this stage of the game. There are many examples of this dynamic already:\n● Faced with a disruptive new technology in the form of Stable Diffusion threatening to obsolete their Content Aware Infill product released in 2018, Adobe, within 8 months managed to integrate the technology into their flagship product, ‘Photoshop’ in form of Generative Fill and adopt the technology into new product lines (Firefly) taking advantage of existing company assets like stock images.\nAs tooling for diffusion based workflows is still in its infancy, existing Adobe customers now have very little incentive to leave the familiarity of their product.\n● While startups like Otter.ai were able to attract high valuation and investment by quickly adopting revolutionary technology like whisper and chatGPT to provide meeting transcription and summarization service, and produce impressive growth in the short term, they now face an uphill battle to grow to their investors expectations: Companies like Zoom, Google (Google Meets) and Microsoft (Teams) all have since announced or launched meeting AI based transcription and summarisation services directly in platform, creating an uphill battle.\n● Thin-layer-over-(chatgpt | stable diffusion) Products like SlidesGPT (AI aided presentation generation), AI assisted letter writing, etc all face a short to mid term terminal event of incumbents integrating AI (well) into their products - such as AI in powerpoint and aggressive competition with a deluge of similar products launched weeks apart.\n● OpenAI’s entire GTM strategy is predicated on betting on incumbents (See Part 2), artificially reinforcing this effect.\n   \n It is hard to not see WhatsApp and other messaging apps ultimately capturing much of AI Chatbot traffic within their universal surfaces, continuing Silicon Valley platform supremacy / taxation for the next decade. LuzIA, a Spanish AI Startup operating a WhatsApp only conversational AI bot has reportedly breached the 1M DAU mark only a few months after launch. (See also Part 2: Meta)\nFaced with this reality, policy makers will have to find the right balance between empowering already powerful quasi monopolies at the centre of information technology and creating opportunities for value creation at the edges of the network.\nAs a result of these dynamics, we suggest that ventures predicated on dislodging existing users from large, well provisioned incumbents are highly risky and likely to fail long term given the likely inability to achieve technological moats unless the incumbent. That is not to say that short term plays by small, agile players cannot exit successfully, especially by selling to incumbents wishing to accelerate their adoption.\nThe Changing Value of Data\nIncumbents with existing audiences and surfaces have another advantage making it hard to assail their position: Data. Much of the promise of generative AI is predicated on available high quality, purpose specific data and successful incumbents either already have or have the option in short order to generate this data from existing product users.\nThe most obvious demonstration of this advantage is Microsoft. Already having much, high quality customer data in their Azure Cloud and Office deployments and having spent many years building regulatory relationships, data residency and privacy compliance their ability to offer cross-product AI finetuning on existing customer data such as email, documents and spreadsheets in the future will enable them to deliver value few other companies could hope to achieve.\nAlready, by June 2023, we are seeing massive shifts in corporate narrative, heralding significant changes and likely the end of the current version of the open web in the long run.:\n● API pricing for search (e.g. Google, SERP) are almost an order of magnitude higher than, for example ChatGPT. (5000 searches = 50$). In fact, programmatic API access to most services users are currently enjoying “for free” with ads are priced at prohibitive levels.\n  \n ● Reddit is engaging in a war with their own volunteer moderator community and third party app developers over API access and prohibitive pricing.\n● Elon Musk taking Twitter ‘private’, removing APIs and ultimately blocking access to the platform for non-signed users from the public web citing ‘illicit data scraping’.\n● Quality repositories of open access data, such as archive.org, have reported large scale attempts at scraping data, putting critical strain on their infrastructure and ability to deliver on their mission, raising serious sustainability questions about destinations currently considered key data sources for model development.\n“Data” is undoubtedly the fuel required to construct advanced generative AI models - but as the above cases show, the devil may be in the details. While the raw size of datasets may have been effective at constructing the current state of technology, forward trends already point into different directions:\na) Quality of datasets is increasingly becoming the differentiating factor for model performance. There is mounting evidence and commentary from insiders to believe that the technology has reached a point of diminishing returns on the size to performance axis. Currently (unverified) speculation about GPT4, the most capable model in the market as of June 2023, is that it uses a Mixtures of Experts (MoE) architecture of multiple smaller models rather than the one-size-fits-all model architecture of the previous generation.\nb) The currently accepted “Chinchilla Data Scaling Law” makes the case that data but not model size is the key to optimal performance of Large Language Models (LLM) - in short, quality and variety being significantly more important than volume. This has significant implications for the often quoted risk of “running out of data” when training more advanced models: Quality of data being a key competitive advantage, not quantity.\nc) Initially demonstrated by Stanford’s Alpaca Experiment and recently reinforced in several papers, including “Textbooks is all you need” (June 2023), use of synthetic data (generated by LLMs) can lead to significant improvements (and massive time/cost savings) vs human labelled data. And while there are concerns about loss of performance due to increasing inability to tell human and AI content apart, it appears likely that synthetic data-sources will play an increasingly important role in the future - potentially devaluing lower quality human datasets.\n          \n Proven Value\nThere is little doubt that quality data is going to be a key, competitive advantage in the future and lack of access to such data can be considered a serious risk for any venture with serious AI ambition. For example:\n● As Bloomberg GPT, SourceGraph Cody and a host of other ‘augmented, specialised models’ show - a company possessing a high value, unique dataset can leverage that data into a competitive advantage in the field.\n● Adobe demonstrated with their Firefly indemnity clause (June 2023), the mere ownership of a dataset with clear licensing status is a competitive advantage in a confusing, uncertain legal and regulatory environment. Many companies are hesitant to adopt generative AI, spooked by the deluge of lawsuits, copyright infringement claims and pending regulatory showdowns. (Also see our paper: Key regulatory and legal developments in Generative AI)\n● GPT3.5 to GPT4 performance gain on tasks in the legal domain demonstrates just how much of a difference model augmentation with a high quality dataset can make, moving the technology from a 10% pass rate on the standard American Bar exam into the 90th percentile.\nNot all data is valuable\nCircling back to Twitter - there is little in fact indication (and much doubt based on existing track-record) that Mr. Musk’s public statements around the value of Twitter data are factual. The more likely explanations for Twitters ‘go private’ moment are temporary metrics juicing to create a temporary spike in sign ups and conversion as well as a vested interest in manufacturing an investor focused “we own valuable data” narrative and using API pricing to define (detached from reality) value of data via platform pricing power.\nThe nature of Twitter’s content itself appears to be of rather limited value to foundational model building too, compared to other, available sources (unless the model was designed to create synthetic content for the platform...). It appears equally likely to us that Reddit is using the controversy around third party APIs to create the appearance of “protecting valuable data for AI” to give the company an “AI paintjob” in view of their IPO plans.\n   The sudden use for many datasets previously considered to merely have curiosity or historic value\n is changing the very nature of the web, bringing change, challenges and opportunities for\n incumbents and new players alike. In the light of these changes, acquiring and developing talent\n able to recognize the value of data has to become a critical priority for any company in 2023.\n \n Not all data is created equal and corporate narratives about the value of data, actions\n supposedly demonstrating that value (“massive data scraping operations targeting our valuable\n data”) or attempts to define prices (at prohibitive levels that are almost certainly not economically\n viable) should not be accepted at face value. Instead, in-depth understanding of the properties of\n data and its relative value in the market is necessary to accurately assess value and potential.\n Changing posture towards data\nAs the technology develops and companies gain better understanding about the role and value of data, we expect significant changes in corporate positioning towards data in the new future:\n● Continued moves away from sharing data openly on the (ad financed) web where it can be scraped and towards locked, pay for access, APIs.\n● The age of seeing data retention as a risk pure and archival as a pure cost centre is over. Demand for clean, licensable multimodal datasets such as artefacts from photo archives, video game asset libraries, books, news archives, audio and movie archives, will drive a fundamental realignment of cost-benefit calculations in this space and, as a side effect, assert pressures on not-for-profit entities like Wikipedia (incredibly valuable for foundational model training), Archive.org and even public libraries.\n● Increased focus on licensing deals and partnerships with generative AI companies or data brokers able to convert dormant data in monetizable assets such as models or lucrative licensing deals. Looking at this Washington Post expose of the most valuable datasets for model development, The News vertical, long struggling with digital, appears overrepresented, indicating potential business opportunity.\n● Large AI companies pursuing exclusivity deals for highly valuable data in an effort to freeze out competitors from key data-sets and build hard to attain moats. Investments into cleanup and curation could offer significant ROI in this regard and may in fact be necessary to overcome pollution of valuable datasets via AI generated content and spam.\n● A focus on collecting current, high quality data about employee and customers interactions with a company’s systems - is critical for any attempt to fine tune generative AI systems for optimal performance on efficiency increasing tasks (or workforce-reduction exercises a la IBM) - and increased tensions with privacy and employee protection regulation complicating the matter.\n     \n ● Regulatory compliance with data, privacy, youth protection, artificial intelligence, medical and other domain specific areas will become an even more current challenge for everyone and, as always, a potential competitive advantage for companies with existing investments in space.\n● Closely related, with increasing value of data, data loss prevention, protection of large datasets in transit for training, protection of proprietary model weights and related areas of cybersecurity and encryption will command increasing attention.\n“There is no (long term, technical) moat in AI”\nWhile we do not agree with the complete premise of the much discussed and undoubtedly not endorsed internal Google position paper titled “We have no moat and neither does OpenAI”, we do consider it critical reading to understand the challenges faced by companies betting on proprietary technology in the generative AI market and many of the alluded effects have already proven themselves in the market, at least when it comes to open source with some of the models have started surpassing proprietary systems like Google Bard in various benchmarks.\nKeeping proprietary knowledge technology secret in the mid or long term is impossible. As already observed, increased talent mobility from Big Tech AI Labs amid structural changes, reduced barriers (like California’s recent ruling to invalidate Non Compete Agreements) and increased opportunity and funding for the technology outside of these labs have made it very hard for companies like Google to retain many of their key researchers over the last year. Even publishing papers without code, a basic necessity for research organisations, now leads to almost instant reproduction of technology via Open Source efforts.\nThe critical part where we disagree with the Google paper is the long term prospect of Open Source beating the top-end models in the field. While proprietary efforts are likely doomed to be replaced with Open Source to take advantage of rapid breakthroughs (as impressively demonstrated by the benefits Meta is reaping from releasing LLaMa), we do believe that the top-of-the-line performing models (like a hypothetical GPT5 or 6) at least in the field of LLMs and multimodal AI will eventually be too computationally expensive to train and operate by anyone but the most well resourced governments and organisations. The technical ability and funding to do so will act as a somewhat effective moat over time. Additionally, there is a high chance that regulatory environments will encourage concentration of these capabilities to few, controllable entities.\n \n In absence of proprietary technology moats maintained by monopolising talent, companies with significant investments requiring shareholder return are turning to other methods of maintaining their lead:\n● Partnerships: OpenAPI’s Go-to-market strategy (Discussed at lengths in part 2) is a testament to the importance of partnership to maximise possible return on investment and put pressure on competitors.\n● Users: As discussed throughout this document, having an active user-base is one of the most significant advantages one can have in AI.\n● Data: Having many users comes with another advantage. OpenAI currently has tens of millions of users providing extremely high value RLHF data (aka teaching GPT4 the necessary questions required to automate knowledge tasks) on a daily basis, especially if one does not have to pay for that data. This is one of the reasons why it would be premature to bet against either Google or Meta in AI - their access to high quality, timely and constantly updated knowledge data creates strong tailwind for their business and their current challenges are more related to the need to carefully shape investor perception and trade off against their primary ads business model than technological.\n● Licensing: From pursuing exclusive licensing to lock competitors, at least temporarily out of high quality, specific datasets to striking deals for such material to provide ‘clean, unencumbered AI’ at a time of uncertainty about copyright related issues, ownership or access to data can provide at least temporary advantages.\n● Lobbying and regulatory capture: Turning current market-leadership and the expert aura that comes with it into regulation that can erect barriers of entry and fortify one’s position is a tried and true industry play and playing out, in real time, in response to Europe’s in-progress AI regulation. (This topic is discussed at length in another paper)\n\n Capitalising on Efficiency Gain\nAt this point it is clear that significant efficiency gains will be unlocked with generative AI adoption across many job groups in knowledge work. This is all the more true when taking into account the rapidly increasing performance of the technology. While studies close to the current state of technology (GPT4) are rare due to the accelerated nature of progress in the field, recent publications have already made the trend clear [1, 2].\nIn this context, we want to draw attention to a recent move by IBM:\nDespite decade-long investments into Watson, IBM ended up with little to show for when it comes to technological leadership. Nevertheless, on May 2nd 2023, they announced some layoffs, a hiring freeze and dangled the possibility of a 30% staff reduction through AI in their back office to investors.\nIf NVIDIA’s recent rise to 1T market valuation marks the point at which most institutional investors made up their mind about AI being a real opportunity (contrasted with Silicon Valley manufactured ones like Metaverse), IBMs announcement could be marking the point at which similar job cut signals will become a regular feature at future earnings seasons.\nGiven IBMs limited growth potential and lack of any credible technological leadership that would enable them to win new market share, we believe that this is an example of investor virtue signalling, promising to allocate the efficiency gains from the adoption of AI to shareholder value - a path undoubtedly followed by many more companies without AI specific growth narratives.\nThis, of course, foreshadows potentially serious challenges: Without regulatory / political intervention, there is little incentive built into the system to not reward shareholders over workers. If even nominally performing companies expect to realise significant efficiency gains and indicate to allocate them to shareholders primarily, it signals the potential for potentially disruptive job market developments fueling social unrest, negative technology sentiment and, in some cases, political instability and impact on consumer purchasing power.\nAnother signal is worthwhile highlighting in this context: Without fail, all Big Technology companies, despite being well positioned to capture value from the rise of generative AI, have laid off a significant number of staff recently. While much of this trend can be attributed to necessary shareholder appeasement given business model pressures (see also Part 2), there may be more to it. It is notable that there appears to be no significant job creation by AI opportunity. And while ‘humans have always found new jobs’ may be true, we’ve never faced the possibility of at scale job-loss without limiting logistical factors before.\n      \n Public Relations Challenges\nGenerative AI already has significant image problems, especially in the west and it is creating adoption constraints and uncertainty for decision makers.\nJust a few recent examples:\n● Marvel faced significant backlash over their AI generated intro sequence to their latest show “Secret Invasion”\n● Ubisoft faced significant PR backlash after announcing an AI aided writing tool.\n● Amnesty International faced backlash for using AI generated images for a campaign.\n● Apple’s WWDC keynotes, which managed to stay clear of the word AI completely despite dense programming is a good indication of that. That’s not to say that AI was not present and centre stage in almost every product: From Neural Chips, Core ML improvements, Machine Learning, Smart Stack, etc, it is clear that Apple intentionally went out of their way to avoid association with the AI hype train some of their most fierce competitors (Google, Facebook) almost desperately tried to summon into existence in their recent presentations and communication.\n● As mentioned before, the fight between Reddit management and much of their own moderation community is, at least to some extent, driven by misaligned messaging around AI. Reddit, looking at an IPO, desperately wants to increase the perceived value of their data by locking it from third party access, thereby infuriating much of their user base.\nIt is fair to say that outside of tech enthusiast and investor circles, the excitement about AI is very limited. Apple clearly understands this and very cleanly delineates their public messaging to consumers (“Don’t mention AI”) from their investor and technology communication (“Deep investments in machine learning, bringing open source AI to Core ML, etc”)\nWe expect these challenges to only grow - most people are already, on some level, spooked by the fear of job loss and competing with untiring robot workers and managing positive sentiment towards AI adoption will be a significant challenge for companies and governments.\n      \n Specifically, the coming US election cycle has the potential to seriously propel AI into the (negative) spotlight. Much breath is spent in public on discussing Alignment of AI, ethical and sustainable AI and short and long term safety concerns, but our cynical read of the topic is that humans are scarcely equipped to align technology when they are not in alignment with themselves, or worse, distracted by fighting culture wars. Specifically the US, the key jurisdiction to watch for handling of AI concerns, seems rather unlikely to act decisively. After all, the prospect of using AI for personalised, post-factual narrative shaping is likely seen as a feature, not a bug by a significant number of legislators.\nOne just has to look at recent, real world examples, to get a glimpse into what to expect in the near future.\nShould, as we believe is likely, job losses intensify with AI adoption, the negative sentiment will pose a serious challenge for companies with aggressive public AI branding and adoption of the technology in general which explains why companies not reliant on AI for their primary growth narrative may be hesitant to get associated with the topic.\n \n  \n The tallest challenge: Staying Current.\nThe most common question we see from decision makers confronted with building AI strategy this year has been “how do you stay on top of it” (which does not even cover understanding where things are going in the mid or long term).\nHands-on interaction with technology, while certainly worthwhile, has limitations, in particular “garbage in / garbage out”: Behind a deceptively simple interface like Chat GPT hides a lot of nuance that, without sufficient understanding, is limiting the value people can get with short term interactions.\nWatching the research seems like a good starting point but is neither scalable nor particularly possible as the speed of developments has long overtaken traditional scientific processes:\n● With 1000s (!!) of preprint papers released every week, few, if any, will be peer reviewed and few people have the time to comb through all of them.\n● Much of the ecosystem has figured out that preprint papers are the most effective way of a press release or investment pitch in AI at this point, so many papers are nothing more than that, but effectively masquerading as research\n● Almost without fail, about 1-2 papers each week represent critical, future defining technology breakthroughs with significant impact on the entire field. Some examples include: ControlNet, LoRA, FlashAttention, AlBI, Colt5, and many more,\nThis, of course, represents an entirely unsatisfactory situation. As a result, ‘show me the code’ has become a somewhat effective mitigation in the technical community. A paper not accompanied by source code on Github.com, PapersWithCode.com or linked to an actual live product/service is unlikely to get any serious attention unless its author has high, established trust in the community.\nIn fact, all of the above linked key papers were released with code and their time to implementation into existing AI tooling and life production could be measured in days, maybe weeks - substitution traditional peer review with hands on demonstration of value.\n        \n While hands on implementation of code is an option for technical companies and experts, it’s not the most scalable option for business decision makers, and so another option is to look at trajectory rather than individual or semi-periodic snapshots:\n● Computers could not do art 18 months ago. 6 months ago they struggled with hands, today they generate at the level of a skilled painter and create visuals rivalling skilled photographers or designers with clear indication that moving images are next.\n● Computers could not code 24 months ago, 12 months ago we got copilot which could synthesise small functions, but starting 3 months ago GPT4 now synthesises complex functions and runs code, fixing mistakes. This month Bard added a coding sandbox to solving high precision mathematical questions with code and specialised models now are capable of creating small apps autonomously.\n● 3 years ago, LLMs started out with autocomplete using a context window of a few 100s characters. At that time, they were barely able to create coherent fanfiction in English. Since then, they rapidly improved into being able to write the most complex prose, detect sarcasm, create satire, epic rap battles, outtranslating google translate and generating risk analysis on Silicon Valley Bank. Their context window has grown to a 100 thousands words, allowing them to answer questions about an entire book they have never been trained on.\n● Speech to Text, a brittle affair since the first SoundBlaster cards (made in Singapore!) graced us with Dr. Sbaitso in the early 90s improved marginally over the years but moved to full on deep fake voice capabilities in the last 18 months, able to create perfect imitations, inflection, speech impediments inclusive with controllable emotion and freely selectable accents.\n● Text to speech, a key focus of research existed equally long but languished at a semi acceptable state in Siri and Google Assistant until a year ago (ironically big tech fired their assistant teams a few weeks before), frustratingly imperfect for people with accents, only for Whisper to enter the scene in September, capable of understanding dozens of languages and almost perfectly transcribing 1 hour of audio in 15 seconds on top end hardware.\n\n And on the technical side:\n● Coming from last year we’ve scaled the possible context window for LLM from 2k to 32k to 100k tokens in implementation and know the path to 1M tokens (massively expanding their possible use cases).\n● We dropped the time to train and fine tune models to 1/3 to 1/7th on most generative technologies in one year while increasing versatility (LoRA).\n● Increasingly replaced humans in the training process, cutting iteration time and cost to new models, with more and more synthetic elements entering training.\n● Saw hardware inference speed increase ~30x between generations (A100-H100), with a cost per hour dropping by 5x\n● Saw the cost of a diffusion model training run drop from 500k USD to ~100k USD in a single year, and the cost of training a GPT3 like to under 500k USD.\n● Saw LLMs moved from Multi-GPU Datacenter rigs with 256GB of Memory all the way down to the Raspberry Pi 4Mb.\nTogether, these real world, working achievements (contrasted with crypto promises of future opportunities) tell us that we have been and continue to experience almost exponential development in Generative AI, enough to at least dismiss some of the concerns around hype and that it would be a mistake to take today’s state of the technology, or its current weaknesses as an indication of where we are in six months or a year.\nIn 2020, the increase in performance gain by AI models was already very noticeable. It also shows that we have entered a new phase of AI development that is very unlike what has come before.\n        \n Summary Part 1 and Closing Thoughts\nThe goal of Part 1 of this paper has been to condense an exceptionally fast moving field encumbered with decades of expectation and narrative into mostly page-sized snapshots ingestible with modest time investment to transfer more-than-skin-deep understanding of the trends and developments driving. We hope it has been successful in that.\nAs always, it is important to keep the exceptional speed of development in generative AI while reading. We suspect that, given previous trends, limitations, challenges and capabilities of these systems will change rapidly, invalidating or accelerating predictions.\nThe best, and only way to make educated decisions and predictions in this field remains staying close to current developments and to avoid hype, misdirection and ideological traps by ruthlessly focusing on applied, observable effects. Until a technological plateau has been reached and software and hardware architectures and use cases have stabilised (a point not currently in sight), the entire field will remain volatile and prone to disruption by breakthrough discoveries.\nWith increased adoption, other effects such safety challenges (US election cycle!), disruption of existing business models and labour market effects will increase pressure for regulatory action and\nComplimentary, our report on the Large Language Model (May) training makes, despite its age in this fast moving market, a good starting point into the more technical aspects of the material. Alternatively, we find a16z’s technical report on Emerging LLM Architectures (June 16, 2023) to be worthwhile reading.\nIn Part 2, we are deep-diving into the key players currently shaping the field, the direction of Generative AI, their strategic positioning and the incentive systems driving their execution.\n \n Appendix: Key Acronyms / Terms\n LLM\nLarge Language Model, Generative AI System operating on human language, for example GPT3 [1]\nRLHF\nReinforcement Learning with Human Feedback, a method of aligning the output of LLMs based on examples of human interactions. [1]\nGPT\nGenerative Pre-trained Transformer, the technical architecture used by the current generation of large language models. Also has become synonymous with OpenAIs product (ChatGPT, etc)\nTransformer\nThe underlying technology of GPT, released in 2017 and partly patented by Google.\nTPU\nTensor Processing Unit, AI chip architecture by Google and currently one of two primary hardware designs used in training Generative AI models\n  GPU\nGraphics Processing Unit originally responsible for outputting accelerated 3D graphics by personal computers, now also the primary hardware devices used in Generative Model Training and inference (e.g. Nvidia GPUs)\n A100 / H100\n  Previous and Current generation State-of-the-art, AI focused GPUs by Nvidia used in generative AI training and inference.\n          \n Appendix: Relevant Models\nA (rapidly evolving) list of relevant models. The current SOTA models are marked with\nFoundational LLM Models\nWe only include models released since 2022 in this list (e.g. no GPT2/KoboldAI/etc).\n         Name\nStatus\nPublisher\nType\n GPT4\nProprietary\nOpenAI\nGeneralist/Chat\n GPT3.5 (ChatGPT)\nProprietary\nOpenAI\nGeneralist/Chat\nClaude\nProprietary\nAnthropic\nGeneralist/Chat\nBard\nProprietary\nGoogle\nGeneralist/Chat\nLLaMa\nOpen Source\nMeta\nGeneralist\nPaLM\nProprietary\nGoogle\nGeneralist\nFalcon\n Open Source\nTII\nGeneralist\nLaMBDa\nUnreleased\nGoogle\nGeneralist\n           Note: Open Source does not necessarily mean commercially usable. The licensing situation around model weights is complex and emerging. We recommend Chatbot Arena for licence references.\nLLM Derivatives\nUnsurprisingly, we se\n        Vicuna\n LLaMa Family\nOpen Source\nlmsys\nGeneralist/Chat\nStableVicuna\n LLaMa Family\nOpen Source\nStability.ai\n Generalist/Chat\nWizardLM\n LLaMa Family\nOpen Source\nGeneralist/Chat\nRedPajama\n LLaMa Family\nOpen Source\nTogether.xyz\nGeneralist/Chat\nGuanaco\n LLaMa Family\nOpen Source\nTim Dettmers\nGeneralist/Chat\n      \n LLM: Special Purpose Models\n       StarCoder\nLLaMa Family\nProgramming\nCoPilot\nProprietary\nGithub/Microsoft\nProgramming\nCody\nReplit.\nProgramming\nWizardCoder\n Open Source\nProgramming\nStarPII\n Open Source\nPII Detection\nBloombergGPT\nProprietary\nBloomberg\nFinancials\n      Diffusion: Foundational Models\nImage generation models\n Model\nStatus\nPublisher\nComments\nMidJourney 5\nProprietary\nMidjourney\n Extreme quality ,\nHigh accessibility, Medium Flexibility No Customizability\nData: Scraped\nNo Artist Opt-Out\n Stable Diffusion XL\nUnreleased\nStability.ai\n Very High Flexibility Very High quality , High Accessibility\nCan be finetuned Data: Scraped, No artist opt-out\n Stable Diffusion 2.x\nOpen Source\nStability.ai\nHigh Quality,\nHigh flexibility,\nHigh Customizability Low accessibility\nCan be finetuned Data: Scraped (LaionB) No artist opt-out\n  Stable Diffusion 1.5\nOpen Source\nStability.ai\nMedium Quality, Very High Flexibility Medium Accessibility.\n          \n        Extreme Customizability\nNSFW capable\nCan be finetuned\nData: Scraped (LaionB) No artist opt-out\n Firefly\nProprietary\nAdobe\nHigh Quality, High Accessibility, High Flexibility\nTrained on Adobe image stock\nLegal Indemnity Assurance"
        },
        "inputs": {
          "text": {
            "connections": []
          }
        },
        "outputs": {
          "text": {
            "connections": [
              {
                "node": 546,
                "input": "url",
                "data": {}
              }
            ]
          }
        },
        "position": [
          -1218.5081733293596,
          -741.5852139014189
        ],
        "name": "omnitool.input_text"
      },
      "546": {
        "id": 546,
        "data": {
          "documents": "",
          "model": "gpt-3.5-turbo-16k|openai",
          "overwrite": false,
          "prompt": "",
          "temperature": 0,
          "url": "",
          "usage": "query_documents"
        },
        "inputs": {
          "documents": {
            "connections": []
          },
          "url": {
            "connections": [
              {
                "node": 545,
                "output": "text",
                "data": {}
              }
            ]
          },
          "usage": {
            "connections": []
          },
          "prompt": {
            "connections": [
              {
                "node": 543,
                "output": "text",
                "data": {}
              }
            ]
          },
          "temperature": {
            "connections": []
          },
          "model": {
            "connections": []
          },
          "overwrite": {
            "connections": []
          }
        },
        "outputs": {
          "answer": {
            "connections": [
              {
                "node": 547,
                "input": "text",
                "data": {}
              }
            ]
          },
          "documents": {
            "connections": []
          }
        },
        "position": [
          -621.4722791737825,
          -411.82912561690716
        ],
        "name": "omni-extension-document_processing:document_processing.docs_with_gpt"
      },
      "547": {
        "id": 547,
        "data": {
          "fileName": "summary",
          "storageType": "Temporary",
          "text": "",
          "textType": "text/markdown"
        },
        "inputs": {
          "text": {
            "connections": [
              {
                "node": 546,
                "output": "answer",
                "data": {}
              }
            ]
          },
          "fileName": {
            "connections": []
          }
        },
        "outputs": {
          "document": {
            "connections": [
              {
                "node": 548,
                "input": "documents",
                "data": {}
              }
            ]
          }
        },
        "position": [
          -122.97747462169781,
          -300.3556607273556
        ],
        "name": "omnitool.write_document"
      },
      "548": {
        "id": 548,
        "data": {
          "audio": "",
          "documents": "",
          "images": "",
          "object": null,
          "text": "",
          "textType": "text/markdown"
        },
        "inputs": {
          "text": {
            "connections": []
          },
          "images": {
            "connections": []
          },
          "audio": {
            "connections": []
          },
          "documents": {
            "connections": [
              {
                "node": 547,
                "output": "document",
                "data": {}
              }
            ]
          },
          "object": {
            "connections": []
          }
        },
        "outputs": {},
        "position": [
          368.3371694879709,
          -318.69749820169
        ],
        "name": "omnitool.chat_output"
      }
    }
  },
  "api": {
    "fields": {}
  },
  "ui": {},
  "_flags": [
    "owner"
  ]
}